{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e4a5a12-b71d-4124-96ff-71e6549cf0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/E-commerce_chatbot/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|███████████████████████████████████████████████████| 115/115 [00:01<00:00, 59.13it/s]\n",
      "100%|██████████████████████████████████████████████████| 115/115 [00:00<00:00, 300.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vector Search Results:\n",
      "Hit Rate: 1.0000\n",
      "MRR: 1.0000\n",
      "Precision@5: 0.2000\n",
      "NDCG: 1.0000\n",
      "\n",
      "Text Search Results:\n",
      "Hit Rate: 1.0000\n",
      "MRR: 1.0000\n",
      "Precision@5: 0.2000\n",
      "NDCG: 1.0000\n",
      "\n",
      "Vector Search Performance by Query Type:\n",
      "general: Success Rate = 1.0000\n",
      "price: Success Rate = 1.0000\n",
      "color: Success Rate = 1.0000\n",
      "size: Success Rate = 1.0000\n",
      "\n",
      "Text Search Performance by Query Type:\n",
      "general: Success Rate = 1.0000\n",
      "price: Success Rate = 1.0000\n",
      "color: Success Rate = 1.0000\n",
      "size: Success Rate = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from elasticsearch import Elasticsearch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class SearchEvaluator:\n",
    "    def __init__(self, es_client, index_name, model_name='all-MiniLM-L6-v2'):\n",
    "        self.es_client = es_client\n",
    "        self.index_name = index_name\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        \n",
    "    def vector_search(self, query, category=None, max_price=None):\n",
    "        \"\"\"Combined vector search using multiple fields\"\"\"\n",
    "        vector = self.model.encode(query)\n",
    "        \n",
    "        search_query = {\n",
    "            \"size\": 5,\n",
    "            \"query\": {\n",
    "                \"bool\": {\n",
    "                    \"must\": [\n",
    "                        {\n",
    "                            \"script_score\": {\n",
    "                                \"query\": {\"match_all\": {}},\n",
    "                                \"script\": {\n",
    "                                    \"source\": \"\"\"\n",
    "                                        cosineSimilarity(params.query_vector, 'productName_vector') * 0.4 + \n",
    "                                        cosineSimilarity(params.query_vector, 'productDescription_vector') * 0.4 + \n",
    "                                        cosineSimilarity(params.query_vector, 'category_vector') * 0.2 + \n",
    "                                        1.0\n",
    "                                    \"\"\",\n",
    "                                    \"params\": {\n",
    "                                        \"query_vector\": vector\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add filters\n",
    "        if category or max_price:\n",
    "            filters = []\n",
    "            if category:\n",
    "                filters.append({\"term\": {\"category.keyword\": category}})\n",
    "            if max_price:\n",
    "                filters.append({\"range\": {\"price\": {\"lte\": max_price}}})\n",
    "            search_query[\"query\"][\"bool\"][\"filter\"] = filters\n",
    "            \n",
    "        results = self.es_client.search(index=self.index_name, body=search_query)\n",
    "        return [hit['_source'] for hit in results['hits']['hits']]\n",
    "    \n",
    "    def text_search(self, query, category=None, max_price=None):\n",
    "        \"\"\"Text-based search using multi_match\"\"\"\n",
    "        search_query = {\n",
    "            \"size\": 5,\n",
    "            \"query\": {\n",
    "                \"bool\": {\n",
    "                    \"must\": {\n",
    "                        \"multi_match\": {\n",
    "                            \"query\": query,\n",
    "                            \"fields\": [\"productName^3\", \"productDescription\", \"availableColours\", \"sizes\"],\n",
    "                            \"type\": \"best_fields\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add filters\n",
    "        if category or max_price:\n",
    "            filters = []\n",
    "            if category:\n",
    "                filters.append({\"term\": {\"category.keyword\": category}})\n",
    "            if max_price:\n",
    "                filters.append({\"range\": {\"price\": {\"lte\": max_price}}})\n",
    "            search_query[\"query\"][\"bool\"][\"filter\"] = filters\n",
    "            \n",
    "        results = self.es_client.search(index=self.index_name, body=search_query)\n",
    "        return [hit['_source'] for hit in results['hits']['hits']]\n",
    "    \n",
    "    def evaluate_search_method(self, ground_truth, search_method):\n",
    "        \"\"\"Evaluate a search method using multiple metrics\"\"\"\n",
    "        relevance_total = []\n",
    "        detailed_results = []\n",
    "        \n",
    "        for query_data in tqdm(ground_truth):\n",
    "            try:\n",
    "                # Get search results\n",
    "                results = search_method(\n",
    "                    query=query_data['question'],\n",
    "                    category=query_data['category'],\n",
    "                    max_price=query_data.get('max_price')\n",
    "                )\n",
    "                \n",
    "                # Check relevance against ground truth\n",
    "                expected_id = str(query_data['product_id'])\n",
    "                relevance = [str(r['id']) == expected_id for r in results]\n",
    "                relevance.extend([False] * (5 - len(relevance)))  # Pad to 5 results\n",
    "                relevance_total.append(relevance)\n",
    "                \n",
    "                # Store detailed results\n",
    "                detailed_results.append({\n",
    "                    'query': query_data['question'],\n",
    "                    'category': query_data['category'],\n",
    "                    'expected_id': expected_id,\n",
    "                    'expected_name': query_data['product_name'],\n",
    "                    'results': [\n",
    "                        {\n",
    "                            'id': r['id'],\n",
    "                            'name': r['productName'],\n",
    "                            'correct': str(r['id']) == expected_id\n",
    "                        } for r in results\n",
    "                    ]\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing query: {query_data['question']}\")\n",
    "                print(f\"Error details: {str(e)}\")\n",
    "                relevance_total.append([False] * 5)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'hit_rate': self.calculate_hit_rate(relevance_total),\n",
    "            'mrr': self.calculate_mrr(relevance_total),\n",
    "            'precision_at_k': self.calculate_precision_at_k(relevance_total),\n",
    "            'ndcg': self.calculate_ndcg(relevance_total),\n",
    "            'detailed_results': detailed_results\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_hit_rate(relevance_total):\n",
    "        \"\"\"Calculate hit rate (recall@k)\"\"\"\n",
    "        return sum(1 for rel in relevance_total if True in rel) / len(relevance_total)\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_mrr(relevance_total):\n",
    "        \"\"\"Calculate Mean Reciprocal Rank\"\"\"\n",
    "        total_score = 0.0\n",
    "        for relevance in relevance_total:\n",
    "            for rank, is_relevant in enumerate(relevance):\n",
    "                if is_relevant:\n",
    "                    total_score += 1.0 / (rank + 1)\n",
    "                    break\n",
    "        return total_score / len(relevance_total)\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_precision_at_k(relevance_total, k=5):\n",
    "        \"\"\"Calculate Precision@k\"\"\"\n",
    "        return sum(sum(rel[:k]) / k for rel in relevance_total) / len(relevance_total)\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_ndcg(relevance_total):\n",
    "        \"\"\"Calculate Normalized Discounted Cumulative Gain\"\"\"\n",
    "        def dcg(rel):\n",
    "            return sum((2**r - 1) / (log2(i + 2)) for i, r in enumerate(rel))\n",
    "        \n",
    "        from math import log2\n",
    "        ndcg_scores = []\n",
    "        for rel in relevance_total:\n",
    "            dcg_score = dcg(rel)\n",
    "            idcg_score = dcg(sorted(rel, reverse=True))\n",
    "            ndcg_scores.append(dcg_score / idcg_score if idcg_score > 0 else 0)\n",
    "        return sum(ndcg_scores) / len(ndcg_scores)\n",
    "\n",
    "def run_comparison():\n",
    "    # Initialize components\n",
    "    es_client = Elasticsearch('http://localhost:9200')\n",
    "    evaluator = SearchEvaluator(es_client, \"shop_products\")\n",
    "    \n",
    "    # Load ground truth data\n",
    "    ground_truth = pd.read_csv('../data/product_qa_groundtruth.csv').to_dict(orient='records')\n",
    "    \n",
    "    # Evaluate both methods\n",
    "    vector_results = evaluator.evaluate_search_method(ground_truth, evaluator.vector_search)\n",
    "    text_results = evaluator.evaluate_search_method(ground_truth, evaluator.text_search)\n",
    "    \n",
    "    # Print comparison results\n",
    "    print(\"\\nVector Search Results:\")\n",
    "    print(f\"Hit Rate: {vector_results['hit_rate']:.4f}\")\n",
    "    print(f\"MRR: {vector_results['mrr']:.4f}\")\n",
    "    print(f\"Precision@5: {vector_results['precision_at_k']:.4f}\")\n",
    "    print(f\"NDCG: {vector_results['ndcg']:.4f}\")\n",
    "    \n",
    "    print(\"\\nText Search Results:\")\n",
    "    print(f\"Hit Rate: {text_results['hit_rate']:.4f}\")\n",
    "    print(f\"MRR: {text_results['mrr']:.4f}\")\n",
    "    print(f\"Precision@5: {text_results['precision_at_k']:.4f}\")\n",
    "    print(f\"NDCG: {text_results['ndcg']:.4f}\")\n",
    "    \n",
    "    # Analyze results by query type\n",
    "    analyze_results_by_query_type(vector_results['detailed_results'], \"Vector Search\")\n",
    "    analyze_results_by_query_type(text_results['detailed_results'], \"Text Search\")\n",
    "    \n",
    "    return vector_results, text_results\n",
    "\n",
    "def analyze_results_by_query_type(detailed_results, method_name):\n",
    "    \"\"\"Analyze performance by query type\"\"\"\n",
    "    df = pd.DataFrame(detailed_results)\n",
    "    df['query_type'] = df['query'].apply(lambda x: \n",
    "        'price' if any(word in x.lower() for word in ['price', 'cost', 'expensive', 'cheap']) \n",
    "        else 'color' if any(word in x.lower() for word in ['color', 'colour']) \n",
    "        else 'size' if any(word in x.lower() for word in ['size', 'fit']) \n",
    "        else 'general'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{method_name} Performance by Query Type:\")\n",
    "    for query_type in df['query_type'].unique():\n",
    "        type_data = df[df['query_type'] == query_type]\n",
    "        success_rate = sum(1 for results in type_data['results'] \n",
    "                          if any(r['correct'] for r in results)) / len(type_data)\n",
    "        print(f\"{query_type}: Success Rate = {success_rate:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    vector_results, text_results = run_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1433989-4527-4974-8972-8a84f722d6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 115/115 [00:01<00:00, 60.57it/s]\n",
      "100%|███████████████████████████████████████████████████| 115/115 [00:02<00:00, 38.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Search Methods Comparison ===\n",
      "\n",
      "Overall Performance Metrics:\n",
      "                Metric  Vector Search  Text Search\n",
      "0             Hit Rate         1.0000       1.0000\n",
      "1                  MRR         1.0000       1.0000\n",
      "2            Precision         0.4783       0.5232\n",
      "3  Avg Search Time (s)         0.0163       0.0257\n",
      "4       Query Coverage         0.5304       0.4904\n",
      "\n",
      "Performance by Query Type:\n",
      "\n",
      "Price Queries:\n",
      "         Metric  Vector Search  Text Search\n",
      "0  Success Rate            1.0          1.0\n",
      "1   Sample Size           23.0         23.0\n",
      "\n",
      "Color Queries:\n",
      "         Metric  Vector Search  Text Search\n",
      "0  Success Rate            1.0          1.0\n",
      "1   Sample Size           30.0         30.0\n",
      "\n",
      "Size Queries:\n",
      "         Metric  Vector Search  Text Search\n",
      "0  Success Rate            1.0          1.0\n",
      "1   Sample Size           15.0         15.0\n",
      "\n",
      "General Queries:\n",
      "         Metric  Vector Search  Text Search\n",
      "0  Success Rate            1.0          1.0\n",
      "1   Sample Size           47.0         47.0\n",
      "\n",
      "=== Recommendations ===\n",
      "\n",
      "1. Overall Performance: Text Search performs better by 0.00% in hit rate.\n",
      "2. Speed: Vector Search is faster by 0.0094 seconds on average.\n",
      "\n",
      "3. Query Type Analysis:\n",
      "\n",
      "4. Final Recommendation:\n",
      "   Consider trade-off:\n",
      "   - Vector Search: 100.00% accuracy, 0.0163s average time\n",
      "   - Text Search: 100.00% accuracy, 0.0257s average time\n",
      "   Recommendation: Choose based on your priority (speed vs. accuracy)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from elasticsearch import Elasticsearch\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "\n",
    "class AdvancedSearchEvaluator:\n",
    "    def __init__(self, es_client, index_name, model_name='all-MiniLM-L6-v2'):\n",
    "        self.es_client = es_client\n",
    "        self.index_name = index_name\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "    \n",
    "    def vector_search(self, query: str, category: str = None, max_price: float = None) -> tuple[List[Dict], float]:\n",
    "        \"\"\"Vector search with timing measurement\"\"\"\n",
    "        start_time = time.time()\n",
    "        vector = self.model.encode(query)\n",
    "        \n",
    "        search_query = {\n",
    "            \"size\": 5,\n",
    "            \"query\": {\n",
    "                \"bool\": {\n",
    "                    \"must\": [\n",
    "                        {\n",
    "                            \"script_score\": {\n",
    "                                \"query\": {\"match_all\": {}},\n",
    "                                \"script\": {\n",
    "                                    \"source\": \"\"\"\n",
    "                                        cosineSimilarity(params.query_vector, 'productName_vector') * 0.4 + \n",
    "                                        cosineSimilarity(params.query_vector, 'productDescription_vector') * 0.4 + \n",
    "                                        cosineSimilarity(params.query_vector, 'category_vector') * 0.2 + \n",
    "                                        1.0\n",
    "                                    \"\"\",\n",
    "                                    \"params\": {\n",
    "                                        \"query_vector\": vector\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if category or max_price:\n",
    "            filters = []\n",
    "            if category:\n",
    "                filters.append({\"term\": {\"category.keyword\": category}})\n",
    "            if max_price:\n",
    "                filters.append({\"range\": {\"price\": {\"lte\": max_price}}})\n",
    "            search_query[\"query\"][\"bool\"][\"filter\"] = filters\n",
    "        \n",
    "        results = self.es_client.search(index=self.index_name, body=search_query)\n",
    "        search_time = time.time() - start_time\n",
    "        \n",
    "        return [hit['_source'] for hit in results['hits']['hits']], search_time\n",
    "    \n",
    "    def text_search(self, query: str, category: str = None, max_price: float = None) -> tuple[List[Dict], float]:\n",
    "        \"\"\"Text search with timing measurement\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        search_query = {\n",
    "            \"size\": 5,\n",
    "            \"query\": {\n",
    "                \"bool\": {\n",
    "                    \"must\": {\n",
    "                        \"multi_match\": {\n",
    "                            \"query\": query,\n",
    "                            \"fields\": [\"productName^3\", \"productDescription\", \"availableColours\", \"sizes\"],\n",
    "                            \"type\": \"best_fields\",\n",
    "                            \"fuzziness\": \"AUTO\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if category or max_price:\n",
    "            filters = []\n",
    "            if category:\n",
    "                filters.append({\"term\": {\"category.keyword\": category}})\n",
    "            if max_price:\n",
    "                filters.append({\"range\": {\"price\": {\"lte\": max_price}}})\n",
    "            search_query[\"query\"][\"bool\"][\"filter\"] = filters\n",
    "        \n",
    "        results = self.es_client.search(index=self.index_name, body=search_query)\n",
    "        search_time = time.time() - start_time\n",
    "        \n",
    "        return [hit['_source'] for hit in results['hits']['hits']], search_time\n",
    "\n",
    "    def evaluate_search_method(self, ground_truth: List[Dict], search_method, method_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"Comprehensive evaluation of a search method\"\"\"\n",
    "        metrics = {\n",
    "            'hit_rate': [], \n",
    "            'mrr': [], \n",
    "            'precision': [],\n",
    "            'search_times': [],\n",
    "            'query_coverage': [],\n",
    "            'results_by_type': {}\n",
    "        }\n",
    "        \n",
    "        query_types = ['price', 'color', 'size', 'general']\n",
    "        for qt in query_types:\n",
    "            metrics['results_by_type'][qt] = {'success': 0, 'total': 0}\n",
    "        \n",
    "        detailed_results = []\n",
    "        \n",
    "        for query_data in tqdm(ground_truth):\n",
    "            try:\n",
    "                # Get search results and timing\n",
    "                results, search_time = search_method(\n",
    "                    query=query_data['question'],\n",
    "                    category=query_data['category'],\n",
    "                    max_price=query_data.get('max_price')\n",
    "                )\n",
    "                \n",
    "                # Record search time\n",
    "                metrics['search_times'].append(search_time)\n",
    "                \n",
    "                # Calculate relevance\n",
    "                expected_id = str(query_data['product_id'])\n",
    "                relevance = [str(r['id']) == expected_id for r in results]\n",
    "                \n",
    "                # Record metrics\n",
    "                metrics['hit_rate'].append(1 if True in relevance else 0)\n",
    "                \n",
    "                # MRR calculation\n",
    "                mrr_score = 0\n",
    "                for rank, is_relevant in enumerate(relevance):\n",
    "                    if is_relevant:\n",
    "                        mrr_score = 1.0 / (rank + 1)\n",
    "                        break\n",
    "                metrics['mrr'].append(mrr_score)\n",
    "                \n",
    "                # Precision calculation\n",
    "                metrics['precision'].append(sum(relevance) / len(results) if results else 0)\n",
    "                \n",
    "                # Query coverage (percentage of results returned)\n",
    "                metrics['query_coverage'].append(len(results) / 5)\n",
    "                \n",
    "                # Query type analysis\n",
    "                query_type = self._determine_query_type(query_data['question'])\n",
    "                metrics['results_by_type'][query_type]['total'] += 1\n",
    "                if any(relevance):\n",
    "                    metrics['results_by_type'][query_type]['success'] += 1\n",
    "                \n",
    "                # Store detailed results\n",
    "                detailed_results.append({\n",
    "                    'query': query_data['question'],\n",
    "                    'query_type': query_type,\n",
    "                    'expected_id': expected_id,\n",
    "                    'found_correct': any(relevance),\n",
    "                    'rank_if_found': next((i+1 for i, r in enumerate(relevance) if r), None),\n",
    "                    'search_time': search_time,\n",
    "                    'num_results': len(results)\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing query: {query_data['question']}\")\n",
    "                print(f\"Error details: {str(e)}\")\n",
    "        \n",
    "        # Calculate final metrics\n",
    "        final_metrics = {\n",
    "            'method_name': method_name,\n",
    "            'hit_rate': np.mean(metrics['hit_rate']),\n",
    "            'mrr': np.mean(metrics['mrr']),\n",
    "            'precision': np.mean(metrics['precision']),\n",
    "            'avg_search_time': np.mean(metrics['search_times']),\n",
    "            'query_coverage': np.mean(metrics['query_coverage']),\n",
    "            'performance_by_type': {\n",
    "                qt: {\n",
    "                    'success_rate': metrics['results_by_type'][qt]['success'] / metrics['results_by_type'][qt]['total']\n",
    "                    if metrics['results_by_type'][qt]['total'] > 0 else 0,\n",
    "                    'sample_size': metrics['results_by_type'][qt]['total']\n",
    "                }\n",
    "                for qt in query_types\n",
    "            },\n",
    "            'detailed_results': detailed_results\n",
    "        }\n",
    "        \n",
    "        return final_metrics\n",
    "\n",
    "    @staticmethod\n",
    "    def _determine_query_type(query: str) -> str:\n",
    "        \"\"\"Determine the type of query\"\"\"\n",
    "        query = query.lower()\n",
    "        if any(word in query for word in ['price', 'cost', 'expensive', 'cheap', 'rand', '$']):\n",
    "            return 'price'\n",
    "        elif any(word in query for word in ['color', 'colour', 'red', 'blue', 'black', 'white']):\n",
    "            return 'color'\n",
    "        elif any(word in query for word in ['size', 'fit', 'small', 'large', 'medium']):\n",
    "            return 'size'\n",
    "        return 'general'\n",
    "\n",
    "def print_comparison_results(vector_metrics: Dict, text_metrics: Dict):\n",
    "    \"\"\"Print detailed comparison of search methods\"\"\"\n",
    "    print(\"\\n=== Search Methods Comparison ===\\n\")\n",
    "    \n",
    "    # Create comparison table\n",
    "    metrics_table = pd.DataFrame({\n",
    "        'Metric': ['Hit Rate', 'MRR', 'Precision', 'Avg Search Time (s)', 'Query Coverage'],\n",
    "        'Vector Search': [\n",
    "            vector_metrics['hit_rate'],\n",
    "            vector_metrics['mrr'],\n",
    "            vector_metrics['precision'],\n",
    "            vector_metrics['avg_search_time'],\n",
    "            vector_metrics['query_coverage']\n",
    "        ],\n",
    "        'Text Search': [\n",
    "            text_metrics['hit_rate'],\n",
    "            text_metrics['mrr'],\n",
    "            text_metrics['precision'],\n",
    "            text_metrics['avg_search_time'],\n",
    "            text_metrics['query_coverage']\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    print(\"Overall Performance Metrics:\")\n",
    "    print(metrics_table.round(4))\n",
    "    \n",
    "    print(\"\\nPerformance by Query Type:\")\n",
    "    for query_type in ['price', 'color', 'size', 'general']:\n",
    "        print(f\"\\n{query_type.title()} Queries:\")\n",
    "        vector_perf = vector_metrics['performance_by_type'][query_type]\n",
    "        text_perf = text_metrics['performance_by_type'][query_type]\n",
    "        \n",
    "        type_table = pd.DataFrame({\n",
    "            'Metric': ['Success Rate', 'Sample Size'],\n",
    "            'Vector Search': [\n",
    "                vector_perf['success_rate'],\n",
    "                vector_perf['sample_size']\n",
    "            ],\n",
    "            'Text Search': [\n",
    "                text_perf['success_rate'],\n",
    "                text_perf['sample_size']\n",
    "            ]\n",
    "        })\n",
    "        print(type_table.round(4))\n",
    "    \n",
    "    # Provide recommendations\n",
    "    print(\"\\n=== Recommendations ===\")\n",
    "    \n",
    "    # Compare overall performance\n",
    "    if vector_metrics['hit_rate'] > text_metrics['hit_rate']:\n",
    "        better_method = \"Vector Search\"\n",
    "        margin = vector_metrics['hit_rate'] - text_metrics['hit_rate']\n",
    "    else:\n",
    "        better_method = \"Text Search\"\n",
    "        margin = text_metrics['hit_rate'] - vector_metrics['hit_rate']\n",
    "    \n",
    "    print(f\"\\n1. Overall Performance: {better_method} performs better by {margin:.2%} in hit rate.\")\n",
    "    \n",
    "    # Compare search times\n",
    "    time_diff = vector_metrics['avg_search_time'] - text_metrics['avg_search_time']\n",
    "    faster_method = \"Vector Search\" if time_diff < 0 else \"Text Search\"\n",
    "    print(f\"2. Speed: {faster_method} is faster by {abs(time_diff):.4f} seconds on average.\")\n",
    "    \n",
    "    # Analyze query type strengths\n",
    "    print(\"\\n3. Query Type Analysis:\")\n",
    "    for query_type in ['price', 'color', 'size', 'general']:\n",
    "        vector_rate = vector_metrics['performance_by_type'][query_type]['success_rate']\n",
    "        text_rate = text_metrics['performance_by_type'][query_type]['success_rate']\n",
    "        better = \"Vector Search\" if vector_rate > text_rate else \"Text Search\"\n",
    "        diff = abs(vector_rate - text_rate)\n",
    "        if diff > 0.05:  # Only mention significant differences\n",
    "            print(f\"   - For {query_type} queries: {better} performs better by {diff:.2%}\")\n",
    "    \n",
    "    # Final recommendation\n",
    "    print(\"\\n4. Final Recommendation:\")\n",
    "    if vector_metrics['hit_rate'] > text_metrics['hit_rate'] and vector_metrics['avg_search_time'] <= text_metrics['avg_search_time']:\n",
    "        print(\"   Use Vector Search - Better accuracy and comparable or better speed\")\n",
    "    elif text_metrics['hit_rate'] > vector_metrics['hit_rate'] and text_metrics['avg_search_time'] <= vector_metrics['avg_search_time']:\n",
    "        print(\"   Use Text Search - Better accuracy and comparable or better speed\")\n",
    "    else:\n",
    "        print(f\"   Consider trade-off:\")\n",
    "        print(f\"   - Vector Search: {vector_metrics['hit_rate']:.2%} accuracy, {vector_metrics['avg_search_time']:.4f}s average time\")\n",
    "        print(f\"   - Text Search: {text_metrics['hit_rate']:.2%} accuracy, {text_metrics['avg_search_time']:.4f}s average time\")\n",
    "        if abs(vector_metrics['hit_rate'] - text_metrics['hit_rate']) < 0.05:\n",
    "            print(\"   Recommendation: Choose based on your priority (speed vs. accuracy)\")\n",
    "\n",
    "def run_advanced_comparison():\n",
    "    \"\"\"Run the advanced comparison\"\"\"\n",
    "    # Initialize components\n",
    "    es_client = Elasticsearch('http://localhost:9200')\n",
    "    evaluator = AdvancedSearchEvaluator(es_client, \"shop_products\")\n",
    "    \n",
    "    # Load ground truth data\n",
    "    ground_truth = pd.read_csv('../data/product_qa_groundtruth.csv').to_dict(orient='records')\n",
    "    \n",
    "    # Evaluate both methods\n",
    "    vector_metrics = evaluator.evaluate_search_method(\n",
    "        ground_truth, evaluator.vector_search, \"Vector Search\"\n",
    "    )\n",
    "    text_metrics = evaluator.evaluate_search_method(\n",
    "        ground_truth, evaluator.text_search, \"Text Search\"\n",
    "    )\n",
    "    \n",
    "    # Print comparison results\n",
    "    print_comparison_results(vector_metrics, text_metrics)\n",
    "    \n",
    "    return vector_metrics, text_metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    vector_metrics, text_metrics = run_advanced_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "991062f4-a9d9-4d36-bf87-81fd512a26e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: multi-qa-MiniLM-L6-cos-v1\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "What would you like to know about our products? (type 'exit' to quit):  hey i have 900 what shoes can i buy \n",
      "Specify category (press Enter to skip):  shoes\n",
      "Maximum price (press Enter to skip):  1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 67.31it/s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/shop_products/_search [status:200 duration:0.004s]\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: With a budget of $900, you have plenty of options. One great choice is our Platform Canvas Sneakers (CLT012) which are currently discounted to $559.99 (20% off the original price of $699.99).\n",
      "\n",
      "These trendy sneakers come in four stylish colors: White, Black, Navy, and Red. They're available in sizes UK4 to UK8, with a regular fit and a comfortable 4cm platform height. \n",
      "\n",
      "If you're looking for something similar, I don't have other options to suggest at the moment, but I highly recommend checking these out. Would you like to know more about this product or would you like to proceed with the purchase?\n",
      "\n",
      "Relevant Products:\n",
      "- Platform Canvas Sneakers ($699.99) [ID: CLT012]\n",
      "\n",
      "Response Time: 1.44 seconds\n",
      "Total Tokens Used: 522\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Was this response helpful? (1: Yes, -1: No):  1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 203\u001b[0m\n\u001b[1;32m    200\u001b[0m             rag_system\u001b[38;5;241m.\u001b[39msave_feedback(result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mint\u001b[39m(feedback))\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 176\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mWhat would you like to know about our products? (type \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mexit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m to quit): \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/E-commerce_chatbot/venv/lib/python3.12/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/E-commerce_chatbot/venv/lib/python3.12/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import uuid\n",
    "from typing import Dict, Any, Tuple, List\n",
    "import logging\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from elasticsearch import Elasticsearch\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "load_dotenv()\n",
    "\n",
    "class EcommerceRAG:\n",
    "    def __init__(self):\n",
    "        self.model_name = os.getenv('MODEL_NAME', 'all-MiniLM-L6-v2')\n",
    "        self.es_url = os.getenv('ELASTICSEARCH_URL', 'http://localhost:9200')\n",
    "        self.groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "        self.index_name = \"shop_products\"\n",
    "        \n",
    "        self.embedding_model = SentenceTransformer(self.model_name)\n",
    "        self.es_client = Elasticsearch(self.es_url)\n",
    "        self.llm_client = Groq()\n",
    "        \n",
    "    def vector_search(self, query: str, category: str = None, max_price: float = None) -> List[Dict]:\n",
    "        \"\"\"Enhanced vector search for product information\"\"\"\n",
    "        vector = self.embedding_model.encode(query)\n",
    "        \n",
    "        search_query = {\n",
    "            \"size\": 5,\n",
    "            \"query\": {\n",
    "                \"bool\": {\n",
    "                    \"must\": [\n",
    "                        {\n",
    "                            \"script_score\": {\n",
    "                                \"query\": {\"match_all\": {}},\n",
    "                                \"script\": {\n",
    "                                    \"source\": \"\"\"\n",
    "                                        cosineSimilarity(params.query_vector, 'productName_vector') * 0.4 + \n",
    "                                        cosineSimilarity(params.query_vector, 'productDescription_vector') * 0.4 + \n",
    "                                        cosineSimilarity(params.query_vector, 'category_vector') * 0.2 + \n",
    "                                        1.0\n",
    "                                    \"\"\",\n",
    "                                    \"params\": {\n",
    "                                        \"query_vector\": vector\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if category or max_price:\n",
    "            filters = []\n",
    "            if category:\n",
    "                filters.append({\"term\": {\"category.keyword\": category}})\n",
    "            if max_price:\n",
    "                filters.append({\"range\": {\"price\": {\"lte\": max_price}}})\n",
    "            search_query[\"query\"][\"bool\"][\"filter\"] = filters\n",
    "        \n",
    "        results = self.es_client.search(index=self.index_name, body=search_query)\n",
    "        return [hit['_source'] for hit in results['hits']['hits']]\n",
    "\n",
    "    def build_product_context(self, products: List[Dict]) -> str:\n",
    "        \"\"\"Builds a context string from retrieved products\"\"\"\n",
    "        context_parts = []\n",
    "        for product in products:\n",
    "            context = f\"\"\"\n",
    "Product: {product['productName']}\n",
    "ID: {product['id']}\n",
    "Price: ${product['price']}\n",
    "Category: {product['category']}\n",
    "Description: {product['productDescription']}\n",
    "Available Colors: {', '.join(product['availableColours'])}\n",
    "Available Sizes: {', '.join(product['sizes'])}\n",
    "\"\"\"\n",
    "            if product.get('discount'):\n",
    "                context += f\"Discount: {product['discount']}%\\n\"\n",
    "            context_parts.append(context.strip())\n",
    "        \n",
    "        return \"\\n\\n\".join(context_parts)\n",
    "\n",
    "    def build_prompt(self, query: str, search_results: List[Dict]) -> str:\n",
    "        \"\"\"Builds a prompt for the e-commerce assistant\"\"\"\n",
    "        prompt_template = \"\"\"\n",
    "You are a knowledgeable and helpful e-commerce shopping assistant. Your role is to help customers find products and answer questions about them based on the CONTEXT provided.\n",
    "\n",
    "Guidelines:\n",
    "- Be concise and friendly in your responses\n",
    "- Include specific product details (name, price, colors, sizes) when relevant\n",
    "- If asked about price, always mention the exact price and any discounts\n",
    "- For sizing questions, mention available sizes and any fit details from the description\n",
    "- If the customer asks about something not in the CONTEXT, politely explain that you can only provide information about available products\n",
    "- If multiple relevant products are found, briefly mention alternatives\n",
    "\n",
    "Customer Question: {question}\n",
    "\n",
    "Available Product Information:\n",
    "{context}\n",
    "\n",
    "Please provide a helpful response based solely on the provided product information.\n",
    "\"\"\".strip()\n",
    "        \n",
    "        context = self.build_product_context(search_results)\n",
    "        return prompt_template.format(question=query, context=context)\n",
    "\n",
    "    def generate_response(self, prompt: str) -> Tuple[str, Dict[str, Any], float]:\n",
    "        \"\"\"Generates a response using the LLM\"\"\"\n",
    "        start_time = time.time()\n",
    "        response = self.llm_client.chat.completions.create(\n",
    "            model='llama-3.1-70b-versatile',\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        response_time = time.time() - start_time\n",
    "        \n",
    "        return (\n",
    "            response.choices[0].message.content,\n",
    "            response.usage.to_dict(),\n",
    "            response_time\n",
    "        )\n",
    "\n",
    "    def handle_query(self, query: str, category: str = None, max_price: float = None) -> Dict[str, Any]:\n",
    "        \"\"\"Main method to handle a customer query\"\"\"\n",
    "        conversation_id = str(uuid.uuid4())\n",
    "        \n",
    "        # Get relevant products\n",
    "        search_results = self.vector_search(query, category, max_price)\n",
    "        \n",
    "        # Build and send prompt to LLM\n",
    "        prompt = self.build_prompt(query, search_results)\n",
    "        answer, tokens, response_time = self.generate_response(prompt)\n",
    "        \n",
    "        # Prepare response data\n",
    "        response_data = {\n",
    "            \"id\": conversation_id,\n",
    "            \"question\": query,\n",
    "            \"answer\": answer,\n",
    "            \"relevant_products\": [\n",
    "                {\n",
    "                    \"id\": product[\"id\"],\n",
    "                    \"name\": product[\"productName\"],\n",
    "                    \"price\": product[\"price\"]\n",
    "                }\n",
    "                for product in search_results\n",
    "            ],\n",
    "            \"response_time\": response_time,\n",
    "            \"tokens\": tokens,\n",
    "            \"total_products_found\": len(search_results)\n",
    "        }\n",
    "        \n",
    "        return response_data\n",
    "\n",
    "    def save_conversation(self, conversation_data: Dict[str, Any]) -> None:\n",
    "        \"\"\"Save conversation to database\"\"\"\n",
    "        # Implement your database saving logic here\n",
    "        pass\n",
    "\n",
    "    def save_feedback(self, conversation_id: str, feedback: int) -> Dict[str, Any]:\n",
    "        \"\"\"Save user feedback\"\"\"\n",
    "        try:\n",
    "            # Implement your feedback saving logic here\n",
    "            return {\"status\": \"success\", \"message\": \"Feedback saved successfully\"}\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error saving feedback: {str(e)}\")\n",
    "            return {\"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "def main():\n",
    "    # Initialize the RAG system\n",
    "    rag_system = EcommerceRAG()\n",
    "    \n",
    "    # Example usage\n",
    "    while True:\n",
    "        query = input(\"\\nWhat would you like to know about our products? (type 'exit' to quit): \")\n",
    "        if query.lower() == 'exit':\n",
    "            break\n",
    "            \n",
    "        # Optional filters\n",
    "        category = input(\"Specify category (press Enter to skip): \").strip() or None\n",
    "        max_price = input(\"Maximum price (press Enter to skip): \").strip()\n",
    "        max_price = float(max_price) if max_price else None\n",
    "        \n",
    "        # Process query\n",
    "        result = rag_system.handle_query(query, category, max_price)\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\nAnswer:\", result[\"answer\"])\n",
    "        print(\"\\nRelevant Products:\")\n",
    "        for product in result[\"relevant_products\"]:\n",
    "            print(f\"- {product['name']} (${product['price']}) [ID: {product['id']}]\")\n",
    "        \n",
    "        print(f\"\\nResponse Time: {result['response_time']:.2f} seconds\")\n",
    "        print(f\"Total Tokens Used: {result['tokens']['total_tokens']}\")\n",
    "        \n",
    "        # Get feedback\n",
    "        feedback = input(\"\\nWas this response helpful? (1: Yes, -1: No): \")\n",
    "        if feedback in ['1', '-1']:\n",
    "            rag_system.save_feedback(result['id'], int(feedback))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65866eeb-e85e-435a-a1a7-0fa2f220b66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:root:Starting evaluation of gemma-7b-it\n",
      "Evaluating gemma-7b-it:   0%|                                     | 0/115 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'product'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 196\u001b[0m\n\u001b[1;32m    193\u001b[0m     evaluator\u001b[38;5;241m.\u001b[39mvisualize_results(results)\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 196\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 190\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    187\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m GroqModelEvaluator()\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# Run evaluation\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_all_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mground_truth_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# Visualize results\u001b[39;00m\n\u001b[1;32m    193\u001b[0m evaluator\u001b[38;5;241m.\u001b[39mvisualize_results(results)\n",
      "Cell \u001b[0;32mIn[8], line 117\u001b[0m, in \u001b[0;36mGroqModelEvaluator.evaluate_all_models\u001b[0;34m(self, ground_truth)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavailable_models:\n\u001b[1;32m    116\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting evaluation of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 117\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m     all_results[model_name] \u001b[38;5;241m=\u001b[39m results\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# Save intermediate results\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 97\u001b[0m, in \u001b[0;36mGroqModelEvaluator.evaluate_model\u001b[0;34m(self, model_name, ground_truth)\u001b[0m\n\u001b[1;32m     88\u001b[0m results \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestions\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal_answers\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse_times\u001b[39m\u001b[38;5;124m'\u001b[39m: []\n\u001b[1;32m     94\u001b[0m }\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m tqdm(ground_truth, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 97\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_prompt(item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mproduct\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     98\u001b[0m     model_answer, response_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_model_response(prompt, model_name)\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_answer:  \u001b[38;5;66;03m# Only process if we got a valid response\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'product'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from groq import Groq\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import logging\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "load_dotenv()\n",
    "\n",
    "class GroqModelEvaluator:\n",
    "    def __init__(self, embedding_model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.groq_client = Groq(api_key=os.getenv('GROQ_API_KEY'))\n",
    "        self.embedding_model = SentenceTransformer(embedding_model_name)\n",
    "        self.available_models = [\n",
    "            'gemma-7b-it',\n",
    "            'llama-3.1-70b-versatile',\n",
    "            'llama-3.2-3b-preview',\n",
    "            'llama3-70b-8192',\n",
    "            'mixtral-8x7b-32768'\n",
    "        ]\n",
    "        \n",
    "    def build_prompt(self, query: str, product_info: Dict) -> str:\n",
    "        \"\"\"Builds a consistent prompt for all models\"\"\"\n",
    "        prompt_template = \"\"\"\n",
    "Given the following product information, please answer the customer's question.\n",
    "Use only the information provided in the product details.\n",
    "\n",
    "Product Information:\n",
    "{product_info}\n",
    "\n",
    "Customer Question: {question}\n",
    "\n",
    "Please provide a clear and concise answer based solely on the above product information.\n",
    "\"\"\".strip()\n",
    "        \n",
    "        product_details = f\"\"\"\n",
    "Name: {product_info['productName']}\n",
    "Price: ${product_info['price']}\n",
    "Category: {product_info['category']}\n",
    "Description: {product_info['productDescription']}\n",
    "Available Colors: {', '.join(product_info['availableColours'])}\n",
    "Available Sizes: {', '.join(product_info['sizes'])}\n",
    "\"\"\".strip()\n",
    "        \n",
    "        return prompt_template.format(\n",
    "            product_info=product_details,\n",
    "            question=query\n",
    "        )\n",
    "\n",
    "    def get_model_response(self, prompt: str, model_name: str) -> Tuple[str, float]:\n",
    "        \"\"\"Get response from specific Groq model with timing\"\"\"\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            response = self.groq_client.chat.completions.create(\n",
    "                model=model_name,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=512\n",
    "            )\n",
    "            response_time = time.time() - start_time\n",
    "            return response.choices[0].message.content, response_time\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error with model {model_name}: {str(e)}\")\n",
    "            return \"\", time.time() - start_time\n",
    "\n",
    "    def compute_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"Compute cosine similarity between two texts\"\"\"\n",
    "        # Encode texts to vectors\n",
    "        vec1 = self.embedding_model.encode([text1])[0]\n",
    "        vec2 = self.embedding_model.encode([text2])[0]\n",
    "        \n",
    "        # Reshape vectors for cosine_similarity\n",
    "        vec1 = vec1.reshape(1, -1)\n",
    "        vec2 = vec2.reshape(1, -1)\n",
    "        \n",
    "        return cosine_similarity(vec1, vec2)[0][0]\n",
    "\n",
    "    def evaluate_model(self, model_name: str, ground_truth: List[Dict]) -> Dict[str, List]:\n",
    "        \"\"\"Evaluate a single model on ground truth data\"\"\"\n",
    "        results = {\n",
    "            'questions': [],\n",
    "            'original_answers': [],\n",
    "            'model_answers': [],\n",
    "            'similarities': [],\n",
    "            'response_times': []\n",
    "        }\n",
    "        \n",
    "        for item in tqdm(ground_truth, desc=f\"Evaluating {model_name}\"):\n",
    "            prompt = self.build_prompt(item['question'], item['product'])\n",
    "            model_answer, response_time = self.get_model_response(prompt, model_name)\n",
    "            \n",
    "            if model_answer:  # Only process if we got a valid response\n",
    "                similarity = self.compute_similarity(item['answer'], model_answer)\n",
    "                \n",
    "                results['questions'].append(item['question'])\n",
    "                results['original_answers'].append(item['answer'])\n",
    "                results['model_answers'].append(model_answer)\n",
    "                results['similarities'].append(similarity)\n",
    "                results['response_times'].append(response_time)\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def evaluate_all_models(self, ground_truth: List[Dict]) -> Dict[str, Dict]:\n",
    "        \"\"\"Evaluate all models and return combined results\"\"\"\n",
    "        all_results = {}\n",
    "        \n",
    "        for model_name in self.available_models:\n",
    "            logging.info(f\"Starting evaluation of {model_name}\")\n",
    "            results = self.evaluate_model(model_name, ground_truth)\n",
    "            all_results[model_name] = results\n",
    "            \n",
    "            # Save intermediate results\n",
    "            self.save_results(model_name, results)\n",
    "            \n",
    "        return all_results\n",
    "\n",
    "    def save_results(self, model_name: str, results: Dict[str, List]) -> None:\n",
    "        \"\"\"Save evaluation results to CSV\"\"\"\n",
    "        df = pd.DataFrame({\n",
    "            'question': results['questions'],\n",
    "            'original_answer': results['original_answers'],\n",
    "            'model_answer': results['model_answers'],\n",
    "            'similarity': results['similarities'],\n",
    "            'response_time': results['response_times']\n",
    "        })\n",
    "        \n",
    "        df.to_csv(f\"evaluation_results_{model_name}.csv\", index=False)\n",
    "\n",
    "    def visualize_results(self, all_results: Dict[str, Dict]) -> None:\n",
    "        \"\"\"Create visualization of model comparisons\"\"\"\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Plot similarity distributions\n",
    "        plt.subplot(2, 1, 1)\n",
    "        for model_name, results in all_results.items():\n",
    "            sns.kdeplot(results['similarities'], label=model_name)\n",
    "        plt.title('Distribution of Cosine Similarities Across Models')\n",
    "        plt.xlabel('Cosine Similarity')\n",
    "        plt.ylabel('Density')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot average response times\n",
    "        plt.subplot(2, 1, 2)\n",
    "        model_names = []\n",
    "        avg_times = []\n",
    "        avg_similarities = []\n",
    "        \n",
    "        for model_name, results in all_results.items():\n",
    "            model_names.append(model_name)\n",
    "            avg_times.append(np.mean(results['response_times']))\n",
    "            avg_similarities.append(np.mean(results['similarities']))\n",
    "        \n",
    "        x = np.arange(len(model_names))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, avg_times, width, label='Avg Response Time (s)')\n",
    "        plt.bar(x + width/2, avg_similarities, width, label='Avg Similarity')\n",
    "        plt.xticks(x, model_names, rotation=45)\n",
    "        plt.title('Average Response Time and Similarity by Model')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('model_comparison_results.png')\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(\"\\nModel Performance Summary:\")\n",
    "        for model_name in model_names:\n",
    "            results = all_results[model_name]\n",
    "            print(f\"\\n{model_name}:\")\n",
    "            print(f\"Average Similarity: {np.mean(results['similarities']):.3f}\")\n",
    "            print(f\"Average Response Time: {np.mean(results['response_times']):.3f}s\")\n",
    "            print(f\"90th Percentile Similarity: {np.percentile(results['similarities'], 90):.3f}\")\n",
    "\n",
    "def main():\n",
    "    # Load your ground truth data\n",
    "    ground_truth_data = pd.read_csv('../data/product_qa_groundtruth.csv').to_dict('records')\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    evaluator = GroqModelEvaluator()\n",
    "    \n",
    "    # Run evaluation\n",
    "    results = evaluator.evaluate_all_models(ground_truth_data)\n",
    "    \n",
    "    # Visualize results\n",
    "    evaluator.visualize_results(results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15dc710c-587e-4e7d-9efb-c738795b32bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /workspaces/E-commerce_chatbot/venv/lib/python3.12/site-packages (3.9.2)\n",
      "Collecting seaborn\n",
      "  Obtaining dependency information for seaborn from https://files.pythonhosted.org/packages/83/11/00d3c3dfc25ad54e731d91449895a79e4bf2384dc3ac01809010ba88f6d5/seaborn-0.13.2-py3-none-any.whl.metadata\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: pandas in /workspaces/E-commerce_chatbot/venv/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /workspaces/E-commerce_chatbot/venv/lib/python3.12/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /workspaces/E-commerce_chatbot/venv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /workspaces/E-commerce_chatbot/venv/lib/python3.12/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /workspaces/E-commerce_chatbot/venv/lib/python3.12/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in /workspaces/E-commerce_chatbot/venv/lib/python3.12/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /workspaces/E-commerce_chatbot/venv/lib/python3.12/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /workspaces/E-commerce_chatbot/venv/lib/python3.12/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /workspaces/E-commerce_chatbot/venv/lib/python3.12/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /workspaces/E-commerce_chatbot/venv/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /workspaces/E-commerce_chatbot/venv/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /workspaces/E-commerce_chatbot/venv/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /workspaces/E-commerce_chatbot/venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install   matplotlib  seaborn pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a418374-e404-4fe6-9156-38eb758b5661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:root:Starting evaluation of gemma-7b-it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ground Truth Data Inspection:\n",
      "Number of items: 115\n",
      "\n",
      "Keys in first item: dict_keys(['product_id', 'product_name', 'category', 'price', 'colors', 'sizes', 'question', 'answer'])\n",
      "\n",
      "First item content:\n",
      "product_id: CLT001\n",
      "product_name: Premium Egyptian Cotton Oxford Shirt\n",
      "category: shirts\n",
      "price: 699.99\n",
      "colors: White, Light Blue, Pink, Light Grey, Powder Blue\n",
      "sizes: S, M, L, XL, XXL\n",
      "question: What is the ID of the Premium Egyptian Cotton Oxford Shirt?\n",
      "answer: CLT001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating gemma-7b-it:   0%|                                     | 0/115 [00:00<?, ?it/s]INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 74.74it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 66.36it/s]\u001b[A\n",
      "Evaluating gemma-7b-it:   1%|▎                            | 1/115 [00:00<01:03,  1.80it/s]INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 95.08it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 95.64it/s]\u001b[A\n",
      "Evaluating gemma-7b-it:   2%|▌                            | 2/115 [00:00<00:49,  2.26it/s]INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Batches: 100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 106.50it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 86.36it/s]\u001b[A\n",
      "Evaluating gemma-7b-it:   3%|▊                            | 3/115 [00:01<00:46,  2.42it/s]INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 73.38it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 67.51it/s]\u001b[A\n",
      "Evaluating gemma-7b-it:   3%|█                            | 4/115 [00:01<00:44,  2.51it/s]INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 91.51it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 81.11it/s]\u001b[A\n",
      "Evaluating gemma-7b-it:   4%|█▎                           | 5/115 [00:01<00:40,  2.69it/s]INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Batches: 100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 110.83it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 68.66it/s]\u001b[A\n",
      "Evaluating gemma-7b-it:   5%|█▌                           | 6/115 [00:02<00:41,  2.63it/s]INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Batches: 100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 114.53it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 78.67it/s]\u001b[A\n",
      "Evaluating gemma-7b-it:   6%|█▊                           | 7/115 [00:02<00:41,  2.62it/s]INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 98.64it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 85.70it/s]\u001b[A\n",
      "Evaluating gemma-7b-it:   7%|██                           | 8/115 [00:03<00:38,  2.76it/s]INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 77.70it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 61.22it/s]\u001b[A\n",
      "Evaluating gemma-7b-it:   8%|██▎                          | 9/115 [00:03<00:38,  2.77it/s]INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 99.26it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 87.70it/s]\u001b[A\n",
      "Evaluating gemma-7b-it:   9%|██▍                         | 10/115 [00:03<00:39,  2.67it/s]INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Batches: 100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 115.85it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 67.32it/s]\u001b[A\n",
      "Evaluating gemma-7b-it:  10%|██▋                         | 11/115 [00:04<00:38,  2.68it/s]INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 93.03it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 76.12it/s]\u001b[A\n",
      "Evaluating gemma-7b-it:  10%|██▉                         | 12/115 [00:04<00:40,  2.57it/s]INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Batches: 100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 107.71it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 75.23it/s]\u001b[A\n",
      "Evaluating gemma-7b-it:  11%|███▏                        | 13/115 [00:05<00:39,  2.61it/s]INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 78.14it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 66.69it/s]\u001b[A\n",
      "Evaluating gemma-7b-it:  12%|███▍                        | 14/115 [00:05<00:37,  2.69it/s]INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Batches: 100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 105.28it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 72.91it/s]\u001b[A\n",
      "Evaluating gemma-7b-it:  13%|███▋                        | 15/115 [00:05<00:39,  2.52it/s]INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Batches: 100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 114.35it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 68.78it/s]\u001b[A\n",
      "Evaluating gemma-7b-it:  14%|███▉                        | 16/115 [00:06<00:37,  2.64it/s]INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Batches: 100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 120.42it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 60.89it/s]\u001b[A\n",
      "Evaluating gemma-7b-it:  15%|████▏                       | 17/115 [00:06<00:37,  2.58it/s]INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Batches: 100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 112.68it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 83.15it/s]\u001b[A\n",
      "Evaluating gemma-7b-it:  16%|████▍                       | 18/115 [00:06<00:35,  2.70it/s]INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Batches: 100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 105.56it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 65.69it/s]\u001b[A\n",
      "Evaluating gemma-7b-it:  17%|████▋                       | 19/115 [00:07<00:33,  2.82it/s]INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 76.27it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 80.32it/s]\u001b[A\n",
      "Evaluating gemma-7b-it:  17%|████▊                       | 20/115 [00:07<00:35,  2.71it/s]INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 98.79it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 68.16it/s]\u001b[A\n",
      "Evaluating gemma-7b-it:  18%|█████                       | 21/115 [00:07<00:34,  2.74it/s]INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 94.86it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 101.36it/s]\u001b[A\n",
      "Evaluating gemma-7b-it:  19%|█████▎                      | 22/115 [00:08<00:34,  2.71it/s]INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Batches: 100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 118.93it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 104.77it/s]\u001b[A\n",
      "Evaluating gemma-7b-it:  20%|█████▌                      | 23/115 [00:08<00:33,  2.73it/s]INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 89.38it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 93.29it/s]\u001b[A\n",
      "Evaluating gemma-7b-it:  21%|█████▊                      | 24/115 [00:09<00:33,  2.71it/s]INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Batches: 100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 103.27it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 94.26it/s]\u001b[A\n",
      "Evaluating gemma-7b-it:  22%|██████                      | 25/115 [00:09<00:32,  2.76it/s]INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Batches: 100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 112.82it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 70.97it/s]\u001b[A\n",
      "Evaluating gemma-7b-it:  23%|██████▎                     | 26/115 [00:09<00:31,  2.86it/s]INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Batches: 100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 114.63it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 107.54it/s]\u001b[A\n",
      "Evaluating gemma-7b-it:  23%|██████▌                     | 27/115 [00:10<00:32,  2.74it/s]INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Batches: 100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 115.47it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 95.83it/s]\u001b[A\n",
      "Evaluating gemma-7b-it:  24%|██████▊                     | 28/115 [00:10<00:31,  2.74it/s]INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 60.29it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 61.15it/s]\u001b[A\n",
      "Evaluating gemma-7b-it:  25%|███████                     | 29/115 [00:10<00:30,  2.83it/s]INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 72.62it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 93.23it/s]\u001b[A\n",
      "Evaluating gemma-7b-it:  26%|███████▎                    | 30/115 [00:11<00:30,  2.77it/s]INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 97.88it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 94.52it/s]\u001b[A\n",
      "Evaluating gemma-7b-it:  27%|███████▌                    | 31/115 [00:13<01:24,  1.01s/it]INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Batches: 100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 117.66it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 82.45it/s]\u001b[A\n",
      "Evaluating gemma-7b-it:  28%|███████▊                    | 32/115 [00:16<02:01,  1.46s/it]INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Batches: 100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 113.23it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 92.71it/s]\u001b[A\n",
      "Evaluating gemma-7b-it:  29%|████████                    | 33/115 [00:18<02:26,  1.79s/it]INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 92.36it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 92.02it/s]\u001b[A\n",
      "Evaluating gemma-7b-it:  30%|████████▎                   | 34/115 [00:21<02:43,  2.01s/it]INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 67.91it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 84.11it/s]\u001b[A\n",
      "Evaluating gemma-7b-it:  30%|████████▌                   | 35/115 [00:23<02:52,  2.16s/it]INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from groq import Groq\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import logging\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "load_dotenv()\n",
    "\n",
    "class GroqModelEvaluator:\n",
    "    def __init__(self, embedding_model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.groq_client = Groq(api_key=os.getenv('GROQ_API_KEY'))\n",
    "        self.embedding_model = SentenceTransformer(embedding_model_name)\n",
    "        self.available_models = [\n",
    "            'gemma-7b-it',\n",
    "            'llama-3.1-70b-versatile',\n",
    "            'llama-3.2-3b-preview',\n",
    "            'llama3-70b-8192',\n",
    "            'mixtral-8x7b-32768'\n",
    "        ]\n",
    "    \n",
    "    def inspect_ground_truth(self, ground_truth: List[Dict]) -> None:\n",
    "        \"\"\"Debug helper to inspect ground truth data structure\"\"\"\n",
    "        print(\"\\nGround Truth Data Inspection:\")\n",
    "        print(f\"Number of items: {len(ground_truth)}\")\n",
    "        if len(ground_truth) > 0:\n",
    "            print(\"\\nKeys in first item:\", ground_truth[0].keys())\n",
    "            print(\"\\nFirst item content:\")\n",
    "            for key, value in ground_truth[0].items():\n",
    "                print(f\"{key}: {value}\")\n",
    "    \n",
    "    def build_prompt(self, item: Dict) -> str:\n",
    "        \"\"\"Builds a consistent prompt for all models using flat data structure\"\"\"\n",
    "        # Convert comma-separated strings to lists\n",
    "        colors = [c.strip() for c in item['colors'].split(',')] if isinstance(item['colors'], str) else item['colors']\n",
    "        sizes = [s.strip() for s in item['sizes'].split(',')] if isinstance(item['sizes'], str) else item['sizes']\n",
    "        \n",
    "        product_details = f\"\"\"\n",
    "Product ID: {item['product_id']}\n",
    "Name: {item['product_name']}\n",
    "Price: ${item['price']}\n",
    "Category: {item['category']}\n",
    "Available Colors: {', '.join(colors)}\n",
    "Available Sizes: {', '.join(sizes)}\n",
    "\"\"\".strip()\n",
    "        \n",
    "        prompt_template = \"\"\"\n",
    "Given the following product information, please answer the customer's question.\n",
    "Use only the information provided in the product details.\n",
    "\n",
    "Product Information:\n",
    "{product_info}\n",
    "\n",
    "Customer Question: {question}\n",
    "\n",
    "Please provide a clear and concise answer based solely on the above product information.\n",
    "\"\"\".strip()\n",
    "        \n",
    "        return prompt_template.format(\n",
    "            product_info=product_details,\n",
    "            question=item['question']\n",
    "        )\n",
    "\n",
    "    def get_model_response(self, prompt: str, model_name: str) -> Tuple[str, float]:\n",
    "        \"\"\"Get response from specific Groq model with timing\"\"\"\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            response = self.groq_client.chat.completions.create(\n",
    "                model=model_name,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=512\n",
    "            )\n",
    "            response_time = time.time() - start_time\n",
    "            return response.choices[0].message.content, response_time\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error with model {model_name}: {str(e)}\")\n",
    "            return \"\", time.time() - start_time\n",
    "\n",
    "    def compute_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"Compute cosine similarity between two texts\"\"\"\n",
    "        if not text1 or not text2:\n",
    "            return 0.0\n",
    "        \n",
    "        # Encode texts to vectors\n",
    "        vec1 = self.embedding_model.encode([text1])[0]\n",
    "        vec2 = self.embedding_model.encode([text2])[0]\n",
    "        \n",
    "        # Reshape vectors for cosine_similarity\n",
    "        vec1 = vec1.reshape(1, -1)\n",
    "        vec2 = vec2.reshape(1, -1)\n",
    "        \n",
    "        return cosine_similarity(vec1, vec2)[0][0]\n",
    "\n",
    "    def evaluate_model(self, model_name: str, ground_truth: List[Dict]) -> Dict[str, List]:\n",
    "        \"\"\"Evaluate a single model on ground truth data\"\"\"\n",
    "        results = {\n",
    "            'questions': [],\n",
    "            'original_answers': [],\n",
    "            'model_answers': [],\n",
    "            'similarities': [],\n",
    "            'response_times': []\n",
    "        }\n",
    "        \n",
    "        for item in tqdm(ground_truth, desc=f\"Evaluating {model_name}\"):\n",
    "            try:\n",
    "                prompt = self.build_prompt(item)\n",
    "                model_answer, response_time = self.get_model_response(prompt, model_name)\n",
    "                \n",
    "                if model_answer:  # Only process if we got a valid response\n",
    "                    similarity = self.compute_similarity(item['answer'], model_answer)\n",
    "                    \n",
    "                    results['questions'].append(item['question'])\n",
    "                    results['original_answers'].append(item['answer'])\n",
    "                    results['model_answers'].append(model_answer)\n",
    "                    results['similarities'].append(similarity)\n",
    "                    results['response_times'].append(response_time)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing item: {str(e)}\")\n",
    "                logging.error(f\"Item content: {item}\")\n",
    "                continue\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def evaluate_all_models(self, ground_truth: List[Dict]) -> Dict[str, Dict]:\n",
    "        \"\"\"Evaluate all models and return combined results\"\"\"\n",
    "        all_results = {}\n",
    "        \n",
    "        for model_name in self.available_models:\n",
    "            logging.info(f\"Starting evaluation of {model_name}\")\n",
    "            results = self.evaluate_model(model_name, ground_truth)\n",
    "            all_results[model_name] = results\n",
    "            \n",
    "            # Save intermediate results\n",
    "            self.save_results(model_name, results)\n",
    "            \n",
    "        return all_results\n",
    "\n",
    "    def save_results(self, model_name: str, results: Dict[str, List]) -> None:\n",
    "        \"\"\"Save evaluation results to CSV\"\"\"\n",
    "        df = pd.DataFrame({\n",
    "            'question': results['questions'],\n",
    "            'original_answer': results['original_answers'],\n",
    "            'model_answer': results['model_answers'],\n",
    "            'similarity': results['similarities'],\n",
    "            'response_time': results['response_times']\n",
    "        })\n",
    "        \n",
    "        df.to_csv(f\"evaluation_results_{model_name}.csv\", index=False)\n",
    "\n",
    "    def visualize_results(self, all_results: Dict[str, Dict]) -> None:\n",
    "        \"\"\"Create visualization of model comparisons\"\"\"\n",
    "        if not all_results:\n",
    "            logging.warning(\"No results to visualize\")\n",
    "            return\n",
    "            \n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Plot similarity distributions\n",
    "        plt.subplot(2, 1, 1)\n",
    "        for model_name, results in all_results.items():\n",
    "            if results['similarities']:  # Only plot if we have similarities\n",
    "                sns.kdeplot(results['similarities'], label=model_name)\n",
    "        plt.title('Distribution of Cosine Similarities Across Models')\n",
    "        plt.xlabel('Cosine Similarity')\n",
    "        plt.ylabel('Density')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot average response times\n",
    "        plt.subplot(2, 1, 2)\n",
    "        model_names = []\n",
    "        avg_times = []\n",
    "        avg_similarities = []\n",
    "        \n",
    "        for model_name, results in all_results.items():\n",
    "            if results['response_times'] and results['similarities']:\n",
    "                model_names.append(model_name)\n",
    "                avg_times.append(np.mean(results['response_times']))\n",
    "                avg_similarities.append(np.mean(results['similarities']))\n",
    "        \n",
    "        if model_names:\n",
    "            x = np.arange(len(model_names))\n",
    "            width = 0.35\n",
    "            \n",
    "            plt.bar(x - width/2, avg_times, width, label='Avg Response Time (s)')\n",
    "            plt.bar(x + width/2, avg_similarities, width, label='Avg Similarity')\n",
    "            plt.xticks(x, model_names, rotation=45)\n",
    "            plt.title('Average Response Time and Similarity by Model')\n",
    "            plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('model_comparison_results.png')\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(\"\\nModel Performance Summary:\")\n",
    "        for model_name in model_names:\n",
    "            results = all_results[model_name]\n",
    "            print(f\"\\n{model_name}:\")\n",
    "            print(f\"Average Similarity: {np.mean(results['similarities']):.3f}\")\n",
    "            print(f\"Average Response Time: {np.mean(results['response_times']):.3f}s\")\n",
    "            print(f\"90th Percentile Similarity: {np.percentile(results['similarities'], 90):.3f}\")\n",
    "\n",
    "def main():\n",
    "    # Load your ground truth data\n",
    "    ground_truth_data = pd.read_csv('../data/product_qa_groundtruth.csv').to_dict('records')\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    evaluator = GroqModelEvaluator()\n",
    "    \n",
    "    # Debug: Inspect ground truth data\n",
    "    evaluator.inspect_ground_truth(ground_truth_data)\n",
    "    \n",
    "    # Run evaluation\n",
    "    results = evaluator.evaluate_all_models(ground_truth_data)\n",
    "    \n",
    "    # Visualize results\n",
    "    evaluator.visualize_results(results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d30be3f-9d05-40bf-9eac-8282868b8f52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
